{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('standardenv': conda)",
   "metadata": {
    "interpreter": {
     "hash": "96b3fd1559923f45a2a7d38e899d8d549575064782cdf1848d466cdc642efcd4"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter 10 Cont.\n",
    "\n",
    "This is the start of the Tensorboard and tuning hyperparameters."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "\n",
    "x_train_full, x_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train_full, y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_valid = scaler.transform(x_valid)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a callback\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD IT!\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", input_shape=x_train.shape[1:]),\n",
    "    tf.keras.layers.Dense(1) # only one prediction cause we are doing houses\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/30\n",
      "  1/363 [..............................] - ETA: 0s - loss: 5.1102WARNING:tensorflow:From C:\\Users\\sjrus\\miniconda3\\envs\\standardenv\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "  2/363 [..............................] - ETA: 11s - loss: 4.6136WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_train_batch_end` time: 0.0584s). Check your callbacks.\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.2365 - val_loss: 0.5551\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 0s 962us/step - loss: 0.4824 - val_loss: 0.4933\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 0s 913us/step - loss: 0.4470 - val_loss: 0.4805\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 0s 957us/step - loss: 0.4290 - val_loss: 0.4642\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 0s 899us/step - loss: 0.4175 - val_loss: 0.4621\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 0s 952us/step - loss: 0.4129 - val_loss: 0.4533\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 0s 935us/step - loss: 0.4054 - val_loss: 0.4442\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3992 - val_loss: 0.4412\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3943 - val_loss: 0.4372\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 0s 918us/step - loss: 0.3913 - val_loss: 0.4353\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 0s 975us/step - loss: 0.3875 - val_loss: 0.4282\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 0s 930us/step - loss: 0.3829 - val_loss: 0.4239\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 0s 915us/step - loss: 0.3837 - val_loss: 0.4256\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 0s 939us/step - loss: 0.3787 - val_loss: 0.4319\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 0s 980us/step - loss: 0.3810 - val_loss: 0.4237\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 0s 985us/step - loss: 0.3756 - val_loss: 0.4155\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 0s 955us/step - loss: 0.3725 - val_loss: 0.4108\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 0s 936us/step - loss: 0.3713 - val_loss: 0.4394\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 0s 912us/step - loss: 0.3702 - val_loss: 0.4084\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 0s 892us/step - loss: 0.3670 - val_loss: 0.4121\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 0s 996us/step - loss: 0.3652 - val_loss: 0.4750\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 0s 928us/step - loss: 0.3668 - val_loss: 0.4132\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3624 - val_loss: 0.4063\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 0s 961us/step - loss: 0.3619 - val_loss: 0.4032\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3627 - val_loss: 0.4039\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 0s 927us/step - loss: 0.3620 - val_loss: 0.4042\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 0s 933us/step - loss: 0.5226 - val_loss: 0.4373\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 0s 941us/step - loss: 0.3856 - val_loss: 0.4218\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 0s 962us/step - loss: 0.3748 - val_loss: 0.4163\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 0s 991us/step - loss: 0.3711 - val_loss: 0.4144\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=30, \n",
    "    validation_data=(x_valid, y_valid),\n",
    "    callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "ERROR: Timed out waiting for TensorBoard to start. It may still be running as pid 3356."
     },
     "metadata": {}
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}